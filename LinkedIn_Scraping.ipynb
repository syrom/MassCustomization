{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version for Medium article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import *\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.actions.wheel_input import ScrollOrigin\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re as re\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Variable declartations (global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# webdriver options: tbc if necessary\n",
    "options = Options()\n",
    "options.add_argument(\"start-maximized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = \"user.name@company.xyu\"\n",
    "PASSWORD = \"1234_if_you_are_really_stupid\"\n",
    "print(USERNAME)\n",
    "print(PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main keyword for search:\n",
    "###############################################################\n",
    "# keywords4search = ['VP Finance']\n",
    "# keywords4search = ['Director FP&A']\n",
    "keywords4search = ['Director Finance']\n",
    "###############################################################\n",
    "# construct link from keyword input\n",
    "s_search_pos = ''   # search string for position\n",
    "for i in keywords4search:\n",
    "    i2 = i.replace(' ', '%20')\n",
    "    s_search_pos+=(i2)+'%20'\n",
    "s_search_pos=s_search_pos[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################\n",
    "# Further filter criteria for the search:\n",
    "###########################################################################################################\n",
    "s_pos_level = 'f_E=5%2C6'             # f_E: Filter Employment type? -> only Direktoren, VP oder GL-Level\n",
    "s_freshness = 'f_TPR=r2592000'       # code for \"position posted within last month\"\n",
    "s_geo_loc = 'geoId=91000002'         # 'geoId=104008204' = Düsseldorf  # 'geoId=101282230' = Deutschland # 'geoId=91000006' = DACH  # 'geoId=103480659' = NRW \n",
    "                                     # 'geoId=91000006' = Benelux # 'geoId=105072130' = Polen  # 'geoId=106693272' = Schweiz # 'geoId=100506914' = Europe\n",
    "                                     # 'geoId=91000002'= Europäischer Wirtschaftsraum\n",
    "# \"Position\"-filter\n",
    "s_position = 'f_T=1088%2C4866%2C4924%2C3968%2C5%2C127%2C1208%2C68'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a) Class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b) Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_scrap_position_details():\n",
    "    # Details to be scrapped from the individual clicked card aka job posting\n",
    "    JobTitle = 'initial_value'\n",
    "    JobTitleLink = 'initial_value'\n",
    "    CompanyName = 'initial_value'\n",
    "    CompanyLocation = 'initial_value'\n",
    "    CompanySize = 'initial_value'\n",
    "    PositionLevel = 'initial_value'\n",
    "    JobDescDetails = 'initial_value'\n",
    "    \n",
    "    try:\n",
    "     JobTitle = driver.find_element(By.CLASS_NAME,\"jobs-unified-top-card__job-title\").text\n",
    "    except:\n",
    "     JobTitle = \"JobTitle not found\"   \n",
    "    try:\n",
    "        JobTitleLink = driver.find_element(By.CLASS_NAME,\"jobs-unified-top-card__content--two-pane\").find_element(By.TAG_NAME,\"a\").get_attribute('href')\n",
    "    except:\n",
    "       JobTitle = \"JobTitleLink not found\"\n",
    "    try:\n",
    "        CompanyName = driver.find_element(By.CLASS_NAME,\"jobs-unified-top-card__company-name\").find_element(By.TAG_NAME,\"a\").text\n",
    "    except:\n",
    "        CompanyName = \"CompanyName not found\"\n",
    "    try:\n",
    "        CompanyLocation = driver.find_element(By.CLASS_NAME,\"jobs-unified-top-card__bullet\").text\n",
    "    except:\n",
    "        CompanyLocation = \"CompanyLocation not found\"\n",
    " \n",
    "    JobDescElements = driver.find_element(By.CLASS_NAME,\"jobs-description-content__text\").find_elements(By.TAG_NAME,\"span\")\n",
    "    for element in JobDescElements:\n",
    "        try:\n",
    "            if len(element.text) > 50:\n",
    "                JobDescDetails = element.text\n",
    "                break\n",
    "            else:\n",
    "                JobDescDetails = \"These are not the JobDescDetails you were looking for\"\n",
    "        except:\n",
    "            JobDescDetails = \"JobDescDetails not found\"\n",
    "    \n",
    "    try:\n",
    "        CompanyInsights = driver.find_elements(By.CLASS_NAME,'.'.join(\"jobs-unified-top-card__job-insight\".split()))\n",
    "        PositionLevel = CompanyInsights[0].text\n",
    "        CompanySize = CompanyInsights[1].text\n",
    "    except:\n",
    "        PositionLevel = \"PostionLevel not found\"\n",
    "        CompanySize = \"CompanySize not found\"\n",
    "    \n",
    "    l_return = [JobTitle, JobTitleLink, CompanyName, CompanyLocation, CompanySize, PositionLevel, JobDescDetails]\n",
    "    \n",
    "    return (l_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Actual Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a) Set up connection to website with credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use driver to open the link\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "driver.get(\"https://www.linkedin.com/uas/login\")\n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use login credentials to login\n",
    "email=driver.find_element(By.ID, \"username\")\n",
    "email.send_keys(USERNAME)\n",
    "password=driver.find_element(By.ID, \"password\")\n",
    "password.send_keys(PASSWORD)\n",
    "time.sleep(3)\n",
    "password.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b) Getting initial information on search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################################################\n",
    "# Complete search string -> ACTUAL POSITIONS TO BE SCRAPED ARE THE ANSWER TO THE CALL OF THIS URL\n",
    "#####################################################################################################################################\n",
    "s_searchURL = f'https://www.linkedin.com/jobs/search/?&{s_pos_level}&{s_position}&{s_freshness}&{s_geo_loc}&keywords={s_search_pos}'\n",
    "print(s_searchURL)\n",
    "#####################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(s_searchURL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = []     \n",
    "link = driver.current_url\n",
    "driver.get(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiatate l_links with nonsense values, but with the rights length to serve as check value in coming WHILE loop\n",
    "#l_links = [1] * len(driver.find_elements(By.CSS_SELECTOR,'a.disabled.ember-view.job-card-container__link.job-card-list__title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_posts = int(driver.find_element(By.CSS_SELECTOR,'small.display-flex.t-12.t-black--light.t-normal.jobs-search-results-list__text').text.split(' ')[0])\n",
    "print(f'For position {s_search_pos} there are {no_posts} vacancies.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npages = (no_posts//25)+1\n",
    "print(f'No. of pages with results: {npages}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c) Main loop: loop over result pages, force \"slow scrolling\" thru all results and retrieve detail info from all cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, npages*25, 25):\n",
    "    print(\"Page:\",int((i/25)+1),\"of\",npages)\n",
    "    s_searchURL = link+\"&start=\"+str(i)\n",
    "    print(s_searchURL)\n",
    "    driver.get(s_searchURL)\n",
    "    time.sleep(4)\n",
    "    \n",
    "    \n",
    "    # get the number of initially available job cards (e.g. usually only the first 7 are loaded)\n",
    "    # but BREAK the for-loop (aka abandon) in case NO such card is found (this can happen if e.g. the\n",
    "    # number of hits after loading several pages suddenly has become smaller - and as the algorithm\n",
    "    # only remembers the higher, original number of hits (and resulting pages), it would try to load\n",
    "    # a page that does not exist (or rather: that does show no search results) anymore\n",
    "    try:\n",
    "        l_links_short = [1] * len(driver.find_elements(By.CSS_SELECTOR,'a.disabled.ember-view.job-card-container__link.job-card-list__title'))\n",
    "    except:\n",
    "        break\n",
    "    \n",
    "    # find the FIRST job card\n",
    "    origin_elem = driver.find_element(By.CSS_SELECTOR,'a.disabled.ember-view.job-card-container__link.job-card-list__title')\n",
    "    v_while_cycles = 0\n",
    "\n",
    "    # forced slow scrolling in order to force loading of all job cards\n",
    "    while (len(l_links_short) < 25) and (v_while_cycles < 12):\n",
    "        v_while_cycles += 1\n",
    "        ###################################################################################################################\n",
    "        # scroll 300 down from the first job card\n",
    "        scroll_origin = ScrollOrigin.from_element(origin_elem, 0, 300)\n",
    "        # and nw scroll 300 deeper down with every new cycle\n",
    "        ActionChains(driver).scroll_from_origin(scroll_origin, 0, (v_while_cycles * 300)).perform() \n",
    "        ###################################################################################################################\n",
    "        ######################\n",
    "        time.sleep(2)\n",
    "        ######################\n",
    "\n",
    "        l_links_long = []\n",
    "        l_links_short = []\n",
    "        l_titles_short = []\n",
    "        for l in driver.find_elements(By.CSS_SELECTOR,'a.disabled.ember-view.job-card-container__link.job-card-list__title'):\n",
    "            txt_long = str(l.get_attribute('text'))\n",
    "            title_short = txt_long[15:]\n",
    "            title_short = title_short.split(\"\\n\")[0]\n",
    "            l_titles_short.append(title_short)\n",
    "\n",
    "            ref_long = str(l.get_attribute('href'))\n",
    "            ref_short = ref_long.split(\"?\")[0]\n",
    "            l_links_long.append(ref_long)\n",
    "            l_links_short.append(ref_short)\n",
    "\n",
    "        print( f'CONTROL: new number of job links after scrolling {v_while_cycles}: value = {len(l_links_short)}')\n",
    "\n",
    "    print('################################################')\n",
    "    print (l_titles_short)\n",
    "    print (len(driver.find_elements(By.CSS_SELECTOR,'a.disabled.ember-view.job-card-container__link.job-card-list__title')))\n",
    "\n",
    "    # now we're sure to have all max. 25 job cards an need to iterate ofer them in order to click on every single\n",
    "    # one and retrieve the details from the it\n",
    "    for l in driver.find_elements(By.CSS_SELECTOR,'a.disabled.ember-view.job-card-container__link.job-card-list__title'):\n",
    "        try:\n",
    "            l.click()\n",
    "        except:\n",
    "            pass        \n",
    "        time.sleep(random.randint(3,5))\n",
    "        try:\n",
    "            result.append(f_scrap_position_details())\n",
    "        except:\n",
    "            pass\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4d) Bring result into data frame format and export data frame to (Excel) file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result, columns = [\"JobTitle\", \"JobTitleLink\", \"CompanyName\",\"CompanyLocation\", \"CompanySize\", \"PositionLevel\",\"JobDescDetails\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace line return from html via regex\n",
    "df['JobDescDetails'] = df['JobDescDetails'].replace(r'\\s+|\\\\n', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split long link so as to keep only the essential part, the direct linkto the job post\n",
    "df[['DirectJobLink','garbage']] = df['JobTitleLink'].str.split(\"?\", n=1, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['JobTitleLink', 'garbage'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"_LINKEDIN_JOB_POSTINGS.xlsx\")     # no openpyxl ????\n",
    "df.to_excel(\"_LINKEDIN_JOB_POSTINGS.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_ws_01",
   "language": "python",
   "name": "ds_ws_01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
